{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"id":"CjPt-gFW-3zG"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","# Imports\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"e_5goh2RrUVq"},"outputs":[],"source":["def save_model(epoch, model, optimizer, loss, name):\n","  torch.save({\n","    'epoch': epoch,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'loss': loss,\n","  }, name+'.pth')\n","\n","def load_model(model, optimizer, epoch, loss, name):\n","  checkpoint = torch.load(name+'.pth')\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  epoch = checkpoint['epoch']\n","  loss = checkpoint['loss']\n","  return model, optimizer, epoch, loss\n","\n","def loadData(trainPath, testPath, batch_size):\n","\n","  transformer = transforms.Compose([\n","    transforms.Resize((256,256)),  #\n","    #transforms.CenterCrop(224),  #\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","  ])\n","\n","  train_loader = DataLoader(\n","      torchvision.datasets.ImageFolder(trainPath, transform = transformer),\n","      batch_size=batch_size,\n","      shuffle=True,\n","      drop_last=True)\n","\n","  test_loader = DataLoader(\n","      torchvision.datasets.ImageFolder(testPath, transform = transformer),\n","      batch_size=batch_size,\n","      shuffle=False,\n","      drop_last=True)\n","\n","  return train_loader, test_loader"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"InoTPH19rXcI"},"outputs":[],"source":["class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","    #Canales de entrada, Canales de salida, Tama√±o de Kernel, Stride y Padding\n","    self.conv1 = nn.Conv2d(3, 16, 3, 1, 0)\n","    self.conv2 = nn.Conv2d(16, 64, 3, 1, 0)\n","    self.conv3 = nn.Conv2d(64, 128, 3, 1, 0)\n","    self.dropout = nn.Dropout(0.2)\n","    self.fc1 = nn.Linear(128*4*4, 512)\n","    self.fc2 = nn.Linear(512, 256)\n","    \n","  def forward(self, x):\n","    #Entrada (batch, channels, ancho, largo)\n","    # (64, 3, 256, 256)\n","    x = torch.relu(self.conv1(x))\n","    x = torch.max_pool2d(x, 2, 2) #input, kernel_size, stride\n","    # (64, 6, 127, 127)\n","    x = torch.relu(self.conv2(x))\n","    x = torch.max_pool2d(x, 2, 2)\n","    # (64, 16, 62, 62)\n","    x = torch.relu(self.conv3(x))\n","    x = self.dropout(x)\n","    x = torch.max_pool2d(x, 2, 2)\n","    # (64, 64, 30, 30)\n","    x = x.view(-1, 128*4*4)\n","    x = torch.relu(self.fc1(x))\n","    x = torch.relu(self.fc2(x))\n","    x = torch.softmax(x, dim=1)\n","\n","    return x"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Is Torch Cuda Available? True\n","Device: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Is Torch Cuda Available? \" + str(torch.cuda.is_available()))\n","print(\"Device: \" + str(device))"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["trainPath = \"Covid19-dataset/train\"\n","testPath = \"Covid19-dataset/test\"\n","batch_size = 64\n","trainloader, testloader = loadData(trainPath, testPath, batch_size)"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"wOSlKxfTrYdU"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[[-0.1922, -0.2941, -0.4275,  ..., -0.6784, -0.7098, -0.7020],\n","          [-0.2078, -0.3333, -0.4745,  ..., -0.6627, -0.6784, -0.6863],\n","          [-0.2471, -0.3804, -0.4824,  ..., -0.6314, -0.6549, -0.6627],\n","          ...,\n","          [ 0.8118,  0.8118,  0.8588,  ...,  0.4196,  0.4353,  0.3569],\n","          [ 0.8353,  0.8353,  0.8824,  ...,  0.4196,  0.4431,  0.4118],\n","          [ 0.8196,  0.8431,  0.8745,  ...,  0.3098,  0.3804,  0.3412]],\n","\n","         [[-0.1922, -0.2941, -0.4275,  ..., -0.6784, -0.7098, -0.7020],\n","          [-0.2078, -0.3333, -0.4745,  ..., -0.6627, -0.6784, -0.6863],\n","          [-0.2471, -0.3804, -0.4824,  ..., -0.6314, -0.6549, -0.6627],\n","          ...,\n","          [ 0.8118,  0.8118,  0.8588,  ...,  0.4196,  0.4353,  0.3569],\n","          [ 0.8353,  0.8353,  0.8824,  ...,  0.4196,  0.4431,  0.4118],\n","          [ 0.8196,  0.8431,  0.8745,  ...,  0.3098,  0.3804,  0.3412]],\n","\n","         [[-0.1922, -0.2941, -0.4275,  ..., -0.6784, -0.7098, -0.7020],\n","          [-0.2078, -0.3333, -0.4745,  ..., -0.6627, -0.6784, -0.6863],\n","          [-0.2471, -0.3804, -0.4824,  ..., -0.6314, -0.6549, -0.6627],\n","          ...,\n","          [ 0.8118,  0.8118,  0.8588,  ...,  0.4196,  0.4353,  0.3569],\n","          [ 0.8353,  0.8353,  0.8824,  ...,  0.4196,  0.4431,  0.4118],\n","          [ 0.8196,  0.8431,  0.8745,  ...,  0.3098,  0.3804,  0.3412]]],\n","\n","\n","        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n","\n","         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n","\n","         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n","\n","\n","        [[[-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922]],\n","\n","         [[-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922]],\n","\n","         [[-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922],\n","          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9922]]],\n","\n","\n","        ...,\n","\n","\n","        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n","\n","         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          ...,\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n","          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n","\n","         [[-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n","          [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n","          [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n","          ...,\n","          [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n","          [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333],\n","          [-0.7333, -0.7333, -0.7333,  ..., -0.7333, -0.7333, -0.7333]]],\n","\n","\n","        [[[-0.9059, -0.8980, -0.8980,  ..., -0.8431, -0.8431, -0.8431],\n","          [-0.8980, -0.8980, -0.8980,  ..., -0.8431, -0.8431, -0.8431],\n","          [-0.8980, -0.8902, -0.8980,  ..., -0.8431, -0.8353, -0.8431],\n","          ...,\n","          [-0.9608, -0.9608, -0.9529,  ..., -0.9294, -0.9216, -0.9216],\n","          [-0.9529, -0.9451, -0.9451,  ..., -0.9216, -0.9216, -0.9137],\n","          [-0.9451, -0.9294, -0.9373,  ..., -0.9137, -0.9059, -0.9059]],\n","\n","         [[-0.9059, -0.8980, -0.8980,  ..., -0.8431, -0.8431, -0.8431],\n","          [-0.8980, -0.8980, -0.8980,  ..., -0.8431, -0.8431, -0.8431],\n","          [-0.8980, -0.8902, -0.8980,  ..., -0.8431, -0.8353, -0.8431],\n","          ...,\n","          [-0.9608, -0.9608, -0.9529,  ..., -0.9294, -0.9216, -0.9216],\n","          [-0.9529, -0.9451, -0.9451,  ..., -0.9216, -0.9216, -0.9137],\n","          [-0.9451, -0.9294, -0.9373,  ..., -0.9137, -0.9059, -0.9059]],\n","\n","         [[-0.9059, -0.8980, -0.8980,  ..., -0.8431, -0.8431, -0.8431],\n","          [-0.8980, -0.8980, -0.8980,  ..., -0.8431, -0.8431, -0.8431],\n","          [-0.8980, -0.8902, -0.8980,  ..., -0.8431, -0.8353, -0.8431],\n","          ...,\n","          [-0.9608, -0.9608, -0.9529,  ..., -0.9294, -0.9216, -0.9216],\n","          [-0.9529, -0.9451, -0.9451,  ..., -0.9216, -0.9216, -0.9137],\n","          [-0.9451, -0.9294, -0.9373,  ..., -0.9137, -0.9059, -0.9059]]],\n","\n","\n","        [[[-0.9529, -0.9373, -0.9137,  ..., -0.3412, -0.2392, -0.8667],\n","          [-0.9529, -0.9373, -0.9137,  ..., -0.5294, -0.3098, -0.8118],\n","          [-0.9529, -0.9373, -0.9137,  ..., -0.5529, -0.5922, -0.9373],\n","          ...,\n","          [-0.9373, -0.9216, -0.8902,  ..., -1.0000, -0.9922, -0.8275],\n","          [-0.9137, -0.2627, -0.1216,  ..., -1.0000, -1.0000, -0.9922],\n","          [-0.6863, -0.0902, -0.5608,  ..., -1.0000, -1.0000, -1.0000]],\n","\n","         [[-0.9529, -0.9373, -0.9137,  ..., -0.3412, -0.2392, -0.8667],\n","          [-0.9529, -0.9373, -0.9137,  ..., -0.5294, -0.3098, -0.8118],\n","          [-0.9529, -0.9373, -0.9137,  ..., -0.5529, -0.5922, -0.9373],\n","          ...,\n","          [-0.9373, -0.9216, -0.8902,  ..., -1.0000, -0.9922, -0.8275],\n","          [-0.9137, -0.2627, -0.1216,  ..., -1.0000, -1.0000, -0.9922],\n","          [-0.6863, -0.0902, -0.5608,  ..., -1.0000, -1.0000, -1.0000]],\n","\n","         [[-0.9529, -0.9373, -0.9137,  ..., -0.3412, -0.2392, -0.8667],\n","          [-0.9529, -0.9373, -0.9137,  ..., -0.5294, -0.3098, -0.8118],\n","          [-0.9529, -0.9373, -0.9137,  ..., -0.5529, -0.5922, -0.9373],\n","          ...,\n","          [-0.9373, -0.9216, -0.8902,  ..., -1.0000, -0.9922, -0.8275],\n","          [-0.9137, -0.2627, -0.1216,  ..., -1.0000, -1.0000, -0.9922],\n","          [-0.6863, -0.0902, -0.5608,  ..., -1.0000, -1.0000, -1.0000]]]],\n","       device='cuda:0')\n","Conv1\n","MaxPool2d\n","Conv2\n","MaxPool2d\n","Conv3\n","MaxPool2d\n","View\n","FC1\n","FC2\n","Softmax\n","torch.Size([3600, 256])\n","torch.Size([64])\n"]},{"ename":"ValueError","evalue":"Expected input batch_size (3600) to match target batch_size (64).","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[75], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[1;32mc:\\Users\\gilro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\gilro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\gilro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\gilro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mValueError\u001b[0m: Expected input batch_size (3600) to match target batch_size (64)."]}],"source":["# Inicializar el modelo, la funci√≥n de p√©rdida y el optimizador\n","model = CNN()\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","# Entrenamiento del modelo\n","num_epochs = 25\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        print(outputs.size())\n","        print(labels.size())\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 200 == 199:\n","            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n","            running_loss = 0.0\n","\n","print('Finished Training')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO8k71aZgZpdO6dHS+e7Fkh","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
